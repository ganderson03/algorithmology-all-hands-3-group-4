---
author: [Joseph Oforkansi, Hank Gref, Coltin Colucci, Grant Anderson, Gabriel Salvatore, Javier]
title: Collision and Run Time Analysis of Hashing Algorithms
page-layout: full
categories: []
date: "2025-4-22"
date-format: long
toc: true
format:
    html:
        code-links:    
            - text: Github Repository
              icon: github
              href: https://github.com/ganderson03/algorithmology-all-hands-3-group-4
        code-fold: true
        code-summary: "Show the code"

---

## Overview

## Implementation

During this experiment, we tested multiple different hashing algorithms and tracked both the number of collisions and the runtimes. We conducted a doubling experiment, increasing the dataset sizes from 5k to 10k and finally to 20k. This allowed us to identify the time complexities of each algorithm and gain a better understanding of how additional data entries impact the number of collisions.

### Python Hash

### Modulo Hash
```{python}
def simple_modulo_hash(key, modulo=1000):
    # Calculate the sum of ASCII values of the characters in the key
    ascii_sum = sum(ord(char) for char in key)
    # Return the hash value as the modulo of the sum
    return ascii_sum % modulo
```

This function takes two inputs, `key` which is a string and `modulo` as an integer that determines the range of the hash values (default 1000). Each character in the string is converted to its ASCII value using the `ord` function. The sum of the ASCII values is divided by the `modulo` value, and then the remainder is taken. This ensures the hash value is between 0 and the `modulo - 1`, which is important for indexing the hash value inside the bucket. 

```{python}
 # Inline implementation of hashing with the simple modulo-based hash function
    hashed_data = {}
    for key, value in dataset.items():
        h = simple_modulo_hash(key, modulo)  # Use the simple modulo-based hash function
        if h in hashed_data:
            hashed_data[h].append((key, value))  # Handle collisions
        else:
            hashed_data[h] = [(key, value)]
```

The keys in `hashed_data` are the hash values computed from the `simple_modulo_hash` function, and the values in `hashed_data` are lists of tuples. Each tuple contains the original key and its associated value from the dataset.

The algorithm iterates through all key-value pairs in the dataset and computes hash values for the keys using simple_modulo_hash. If a hash value already exists as a key in hashed_data, the key-value pair is appended to the list of items stored under that hash value (h). If the hash value does not exist, a new entry is created in hashed_data with the hash value as the key and the key-value pair as the first item in the list.

The Modulo Hashing algorithm has a time complexity of O(n), also known as linear time. We came to this conclusion by running a doubling experiment using 5k, 10k, and 20k key dictionaries and timing how long it took to hash each. As the number of keys doubled so did the runtimes, confirming the linear time complexity. While showing a mostly linear pattern there was a small uptick in runtimes at each increaseâ€”likely due to a rise in collisions for the larger datasets.

### Murmur Hash

```{python}
def murmurhash(key: str, seed: int = 0) -> int:
    """Compute the MurmurHash for a given string.
    """
    key_bytes = key.encode('utf-8')
    length = len(key_bytes)
    h = seed
    c1 = 0xcc9e2d51
    c2 = 0x1b873593
    r1 = 15
    r2 = 13
    m = 5
    n = 0xe6546b64

    # Process the input in 4-byte chunks
    for i in range(0, length // 4):
        k = int.from_bytes(key_bytes[i * 4:(i + 1) * 4], byteorder='little')
        k = (k * c1) & 0xFFFFFFFF
        k = (k << r1 | k >> (32 - r1)) & 0xFFFFFFFF
        k = (k * c2) & 0xFFFFFFFF

        h ^= k
        h = (h << r2 | h >> (32 - r2)) & 0xFFFFFFFF
        h = (h * m + n) & 0xFFFFFFFF

    # Process the remaining bytes
    remaining_bytes = length & 3
    if remaining_bytes:
        k = int.from_bytes(key_bytes[-remaining_bytes:], byteorder='little')
        k = (k * c1) & 0xFFFFFFFF
        k = (k << r1 | k >> (32 - r1)) & 0xFFFFFFFF
        k = (k * c2) & 0xFFFFFFFF
        h ^= k

    # Finalize the hash
    h ^= length
    h ^= (h >> 16)
    h = (h * 0x85ebca6b) & 0xFFFFFFFF
    h ^= (h >> 13)
    h = (h * 0xc2b2ae35) & 0xFFFFFFFF
    h ^= (h >> 16)

    return h


def load_dataset(file_path: str) -> List[str]:
    """Load a dataset from a file, where each line is treated as a separate entry."""
    with open(file_path, 'r', encoding='utf-8') as file:
        return [line.strip() for line in file]


def hash_dataset(file_path: str, seed: int = 0) -> List[int]:
    """Hash each line of a dataset using MurmurHash."""
    dataset = load_dataset(file_path)
    return [murmurhash(line, seed) for line in dataset]


if __name__ == "__main__":
    # Example usage
    dataset_file = "dataset.txt"  # Replace with the path to your dataset file
    seed_value = 42

    try:
        hashes = hash_dataset(dataset_file, seed_value)
        for i, h in enumerate(hashes, start=1):
            print(f"Line {i}: Hash = {h}")
    except FileNotFoundError:
        print(f"Error: The file '{dataset_file}' was not found.")
```

The implementation of MurmurHash in the provided code is a 32-bit non-cryptographic hash function designed for efficiency and uniform distribution. 

The function begins by encoding the input string into bytes and processing it in 4-byte chunks. Each chunk is transformed using a series of multiplications, bitwise rotations, and masking operations to ensure randomness and minimize collisions. 

Any remaining bytes that do not fit into a 4-byte chunk are processed separately to ensure all input data contributes to the final hash. The hash value is further refined in a finalization step, where it undergoes additional bitwise operations and multiplications to improve distribution. 

The function also incorporates a seed value, allowing for customizable hash outputs. This implementation is particularly effective for applications requiring fast and reliable hashing, such as hash tables or data indexing.

### SHA-256

## Data

```text
| Hash Method | Dataset     | Total Keys | Unique Hash Values | Total Collisions | Avg Time Taken (s) | Seed Value | Modulo Value |
|-------------|-------------|------------|--------------------|------------------|--------------------|------------|--------------|
| builtin     | dataset_5k  | 5000       | 5000               | 0                | 0.0056             | --         | --           |
| builtin     | dataset_10k | 10000      | 10000              | 0                | 0.0109             | --         | --           |
| builtin     | dataset_20k | 20000      | 20000              | 0                | 0.0216             | --         | --           |
| murmur      | dataset_5k  | 1          | 1                  | N/A              | 0.0540             | 1000       | --           |
| murmur      | dataset_10k | 1          | 1                  | N/A              | 0.1075             | 1000       | --           |
| murmur      | dataset_20k | 1          | 1                  | N/A              | 0.2160             | 1000       | --           |
| modulo      | dataset_5k  | 5000       | 376                | 331              | 0.0129             | --         | 1000         |
| modulo      | dataset_10k | 10000      | 406                | 371              | 0.0247             | --         | 1000         |
| modulo      | dataset_20k | 20000      | 436                | 404              | 0.0526             | --         | 1000         |
| djb2        | dataset_5k  | 5000       | 4826               | 174              | 0.0165             | --         | --           |
| djb2        | dataset_10k | 10000      | 9270               | 730              | 0.0316             | --         | --           |
| djb2        | dataset_20k | 20000      | 17293              | 2707             | 0.0675             | --         | --           |
```

## Analysis

## Conclusion

